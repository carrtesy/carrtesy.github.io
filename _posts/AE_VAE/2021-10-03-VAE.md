---
title: "[Deep Learning] Variational Autoencoder"
date: 2021-10-03
last_modified_at: 2021-05-25

categories:
 - Deep Learning 

tags:
 - Deep Learning
 - Generative Models 
use_math: true
---



What is Variational Autoencoder?



## First Thing: Forget Autoencoders!

As I believe, you should forget about autoencoders first. VAE's structure is really similar to that of autoencoders. However, each has different purposes, and different philosophies.

 ![Autoencoder](..\..\assets\images\AE_VAE\AE_FULL.png)



## VAE is a Generative Model

VAE is generative model. That is, VAE is aimed to build a model that generates data that is similar to that of $X$ in the training database. 

Hence, mathematically we are trying to maximize $p(X)$. $p(X)$ means the probability of generated sample $X$ to be in the training set.

$ Max P(X) $

## Assumption 1: There exists some latent vector $z$

Here is the first checkpoint.  Here we assume there exists vector $z$ that inherits all the properties of $X$. 

![vae-latent](..\..\assets\images\AE_VAE\VAE_latent.png)

*$z$ contains the essence of $x$, and that can decide what x is.*

For intuitive sense, $X$ may corresponds to *news*, and $z$ corresponds to the *topic* of the news. That topic $z$ would decide what kind of news $X$ is generated. 

Our objective was to maximize $P(X)$.

For that purpose, we can formulate $P(X)$ using $z$'s.

$P(X) = \int\limits_{z} P(X\|z) P(z) \,dz$



Using our *news/topic* analogy, above term integrates all probability of being specific news, say Lionel messi's homerun news $X$ given the topics (e.g. football, baseball, and so on) $z$.



## Problem 1: Too many of $z$'s!

However, integrating over $z$ is really hard, as there are so many $z$'s possible out there. We say it "intractable". In this world, there are many topics other than just sports! Most of $z$'s would not produce messi's news, which means $p(z) = 0$ for most of the times.



## Solution 1: Pick only feasible $z$'s!

What can we do then?  How about sampling values of $z$ that are likely to have produced $x$, the messi news? We can then compute $P(X)$ from there.

For this purpose, we may build another network $Q(z|X)$ which gets $X$ as an input and give us a distribution of $z$ values that are likely to produce $X$.

Here, our objective has become:

$E_{z \sim Q} {P(X|z)}$




## Problem 2: How can we relate $p(X)$ and $E_{z \sim Q} {P(X|z)}$ ?

The relationship between $p(X)$ and $E_{z \sim Q} {P(X|z)}$ is not quite obvious. Function $q$ has just suddenly came. How can we relate this to our objective?



## Solution 2: 

We can begin with KL-divergence.

KL divergence of $P(z|X)$ and $Q(z|X) $ is as follows:

$D[Q(z|X) || P(z|X)] = E_{z \sim Q}[log Q(z) - logP(z|X)] = E_{z \sim Q}[log Q(z) - logP(X|z) - logP(z) + logP(X)]$

Rearranging the terms,

$logP(X) - D[Q(z|X) || P(z|X)] = E_{z \sim Q}[log P(X|Z)] - D[Q(z|X) || P(z)]$

So our objective of maximizing $P(X)$ can be described as maximizing KL-divergence of $Q(z|X)$ and $P(z|X)$. That can be done by minimizing RHS terms.



## Optimization

Optimization consists of two parts:

First term is $E_{z \sim Q}[log P(X|Z)]$, and second term is  $D[Q(z|X) || P(z)]$



Let's take a look at easier term first, the second term.

Second term is  $$ D[Q(z|X) || P(z)] $$, and we are going to assume that $Q(z|X)$ and $P(z)$ follows gaussian. Luckily, KL-Divergence of two Gaussian PDF has analytical solution.

$D[N(\mu_0, \Sigma_0)||N(\mu_1, \Sigma_1)] = \frac{1}{2}(tr(\Sigma_1^{-1}\Sigma_0) + (\mu_1 - \mu_0)^{T}\Sigma_1^{-1}(\mu_1 - \mu_0)-k+log(\frac{det\Sigma_1}{det\Sigma_0})$

How about the first term?

Getting a good estimate of $E_{z \sim Q}[log P(X|Z)]$ requires many $z$. 

Therefore, what we do is to *sample* $z$ to get a good estimate of the first term.  



## Final Architecture

Final architecture of VAE is as follows. Now, you can think of AE also. AE has exactly the same architecture to that of VAE, except the KL loss term and the sampling process. This makes a huge difference. 

![vae-latent](..\..\assets\images\AE_VAE\VAE_arch.png)

---

This is the 1st draft written at Oct 3, 2021

1st draft: 2021-10-03
