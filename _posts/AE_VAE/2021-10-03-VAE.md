---
title: "[Deep Learning] Variational Autoencoder"
date: 2021-10-03
last_modified_at: 2021-10-10

categories:
 - Deep Learning 

tags:
 - Deep Learning
 - Generative Models
 - VAE
 - AE
use_math: true
---



What is Variational Autoencoder?



## From AE to VAE

VAE is a generative model. VAE is aimed to build a model that generates data similar to that of $X$ in the training database. 

![Autoencoder](..\..\assets\images\AE_VAE\AE_FULL.png)

Consider the autoencoder. Autoencoder does not care about the $z$'s structure. AE just encrypts the data and decrypt afterward. 

However, in order to generate data, we need more firm control over the space of $z$'s. In other words, we want $p(z)$ to follow certain distribution which is really familiar to us (e.g. Gaussian). This is what VAE does. 



## There exists some latent vector $z$

Here is the first checkpoint.  Here we assume that there exists a vector $z$ that inherits all the properties of $X$. 

For variable $z$, we call it "latent". "Latent" is just a fancy way of calling "hidden" - this $z$ "hides" every tedious details of $X$ and gives us the essence of it.

![vae-latent](..\..\assets\images\AE_VAE\VAE_latent.png)

*$z$ contains the essence of $x$, and that can decide what x is.*



Here, we are interested in the value of $ P(z\|X) $, which we call *posterior distribution*. This gives the probability of latent variable $z$ given the evidence $X$.



## Too many of $z$'s!

In order to calculate the *posterior*, we need to set up the formula using the bayesian method. 

$ P(z\|X) = \frac{P(X\|z) P(z)}{P(X)} $

For the denominator, P(X) can be calculated with respect to all possible values of $z$'s.

That is,  $P(X) = \int\limits_{z} P(X|z) P(z) ,dz$.

However, integrating over $z$ is hard, as there are too many $z$'s possible out there. 

For example, if we only pick $z$'s dimension as 2, any points in the whole 2D plane correspond to $z$'s possible value. 

We say this integral is "intractable". Most of $z$'s would not produce the data that we want, which means $p(z) = 0$ for most of the time.



## Bayesian Inference

What can we do then? Here, we can introduce *bayesian inference*.

Here, we try to approximate $P(z\|X)$, not exactly to calculate. This can be done by setting another distribution $Q(z|X)$, and pull $Q(z|X)$ to $P(z|X)$ as much as possible.



### KL-divergence

Difference of the two distributions $Q(z|X)$ and $P(z|X)$ can be calculated using KL-divergence.

For probability distribution A and B, we define KL divergence of B with respect to A as: 

$D_{KL}[A(X)\|\|B(X)] = E{X \sim A}{log(\frac{A}{B})} = E{X \sim A}{log(A) - log(B)} $



Similarly, KL divergence of  $Q(z\|X)$ with respect to $P(z\|X)$ is as follows:

$D_{KL}[Q(z\|X) \|\| P(z\|X)] = E_{z \sim Q(z\|X)}[log Q(z\|X) - logP(z\|X)] $



$= E_{z \sim Q(z\|X)}[log Q(z) - logP(X\|z) - logP(z) + logP(X)] $






## Problem 2: How can we relate $p(X)$ and $E_{z \sim Q} {P(X\|z)}$ ?

The relationship between $p(X)$ and $E_{z \sim Q} {P(X\|z)}$ is not quite obvious. Function $Q$ has just suddenly came. How can we relate this to our objective?



## Solution 2: 

We can begin with KL-divergence.

KL divergence of $P(z\|X)$ and $Q(z\|X) $ is as follows:

$D[Q(z\|X) \|\| P(z\|X)] = E_{z \sim Q}[log Q(z) - logP(z\|X)] = E_{z \sim Q}[log Q(z) - logP(X\|z) - logP(z) + logP(X)]$

Rearranging the terms,

$logP(X) - D[Q(z\|X) \|\| P(z\|X)] = E_{z \sim Q}[log P(X\|Z)] - D[Q(z\|X) \|\| P(z)]$

So our objective of maximizing $P(X)$ can be described as maximizing KL-divergence of $Q(z\|X)$ and $P(z\|X)$. That can be done by minimizing RHS terms.



## Optimization

Optimization consists of two parts:

First term is $E_{z \sim Q}[log P(X\|Z)]$, and second term is  $D[Q(z\|X) \|\| P(z)]$



Let's take a look at the easier term first, the second term.

The second term is  $$ D[Q(z\|X) \|\| P(z)] $$, and we are going to assume that $Q(z\|X)$ and $P(z)$ follows gaussian. Luckily, KL-Divergence of two Gaussian PDF has an analytical solution.

$D[N(\mu_0, \Sigma_0)\|\|N(\mu_1, \Sigma_1)] = \frac{1}{2}(tr(\Sigma_1^{-1}\Sigma_0) + (\mu_1 - \mu_0)^{T}\Sigma_1^{-1}(\mu_1 - \mu_0)-k+log(\frac{det\Sigma_1}{det\Sigma_0})$

How about the first term?

Getting a good estimate of $E_{z \sim Q}[log P(X\|Z)]$ requires many $z$. 

Therefore, what we do is to *sample* $z$ to get a good estimate of the first term.  



## Final Architecture

The final architecture of VAE is as follows. Now, you can think of AE also. AE has the same architecture as that of VAE, except the KL loss term and the sampling process. This makes a huge difference. 

![vae-latent](..\..\assets\images\AE_VAE\VAE_arch.png)

---

This is the 1st draft written on Oct 3, 2021

1st draft: 2021-10-03
